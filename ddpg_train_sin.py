# ddpg_train_stable.py (Modified for Sinusoidal Target Tracking)
import random, math, time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
import matplotlib.pyplot as plt

# device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ---------- Motor ----------
#é€™æ˜¯ä¸€å€‹é›¢æ•£åŒ–çš„äºŒéšç³»çµ±ï¼ˆå·®åˆ†å½¢å¼ï¼‰ç”¨ä¾†æ¨¡æ“¬é¦¬é”å‹•æ…‹ï¼Œåƒæ•¸ï¼š
#Kï¼šå¢ç›Šï¼ˆè¼¸å…¥åˆ°è¼¸å‡ºï¼‰
#tauï¼šæ™‚é–“å¸¸æ•¸ï¼Œå½±éŸ¿ç³»çµ±åæ‡‰é€Ÿåº¦
#zetaï¼šé˜»å°¼æ¯”
#Tsï¼šæ™‚é–“æ­¥é•·ï¼ˆç§’ï¼‰
class DifferentiableMotor:
    def __init__(self, K=10, tau=0.5, zeta=0.7, Ts=0.01):
        self.Ts = Ts; self.K = K; self.tau = tau; self.zeta = zeta
        self.reset()
    def reset(self):
        self.omega = 0.0
        self.omega_prev = 0.0
    def step(self, u):
        # u: control input (voltage)
        a1 = 1 + 2 * (-self.zeta * self.Ts / self.tau)
        a2 = - (self.Ts / self.tau)**2
        b1 = self.K * self.Ts**2 / (self.tau**2)
        omega_next = a1 * self.omega + a2 * self.omega_prev + b1 * u
        self.omega_prev = self.omega
        self.omega = float(omega_next)
        return self.omega

# ---------- Replay Buffer ----------
#å„²å­˜ç¶“é©— (state, action, reward, next_state, done)ï¼Œç”¨ uniform random sampleã€‚
#å›æ”¾å¤§å°ã€batch å¤§å°æœƒå½±éŸ¿è³‡æ–™å¤šæ¨£æ€§èˆ‡æ”¶æ–‚ç©©å®šæ€§ã€‚
Transition = namedtuple("Transition", ["s","a","r","s2","done"])
class ReplayBuffer:
    def __init__(self, maxlen=200000):
        self.buf = deque(maxlen=maxlen)
    def push(self, s,a,r,s2,done):
        self.buf.append(Transition(s,a,r,s2,done))
    def sample(self, batch_size):
        batch = random.sample(self.buf, batch_size)
        s = torch.tensor(np.vstack([b.s for b in batch]), dtype=torch.float32, device=device)
        a = torch.tensor(np.vstack([b.a for b in batch]), dtype=torch.float32, device=device)
        r = torch.tensor(np.vstack([b.r for b in batch]), dtype=torch.float32, device=device)
        s2 = torch.tensor(np.vstack([b.s2 for b in batch]), dtype=torch.float32, device=device)
        d = torch.tensor(np.vstack([b.done for b in batch]).astype(np.float32), dtype=torch.float32, device=device)
        return s,a,r,s2,d
    def __len__(self): return len(self.buf)

# ---------- Networks ----------
def mlp(in_dim, out_dim, hidden=256, final_act=None):
    layers = [nn.Linear(in_dim, hidden), nn.ReLU(),
              nn.Linear(hidden, hidden//2), nn.ReLU(),
              nn.Linear(hidden//2, out_dim)]
    if final_act: layers.append(final_act)
    return nn.Sequential(*layers)

class Actor(nn.Module):
    def __init__(self, state_dim, action_max=24.0):
        super().__init__()
        self.net = mlp(state_dim, 1, hidden=256, final_act=nn.Tanh())
        self.action_max = action_max
    #state
    def forward(self, s):
        #actor ä½¿ç”¨ tanh * action_max å¯ä»¥è‡ªç„¶å°‡å‹•ä½œå£“åˆ°åˆé©ç¯„åœã€‚
        return self.net(s) * self.action_max

class Critic(nn.Module):
    def __init__(self, state_dim):
        super().__init__()
        self.net = mlp(state_dim + 1, 1, hidden=256)
    #state, action
    def forward(self, s, a):
        x = torch.cat([s, a], dim=1)
        return self.net(x)

# ---------- OU noise ----------
#OUActionNoise å¯¦ä½œçš„æ˜¯ Ornsteinâ€“Uhlenbeck (OU) éš¨æ©Ÿéç¨‹ çš„é›¢æ•£è¿‘ä¼¼ï¼Œç”¨ä¾†åœ¨é€£çºŒå‹•ä½œå¼·åŒ–å­¸ç¿’ï¼ˆä¾‹å¦‚ DDPGï¼‰ä¸­ç”¢ç”Ÿæœ‰æ™‚é–“ç›¸é—œæ€§çš„ã€Œå¹³æ»‘ã€ï¼ˆtemporally correlatedï¼‰å™ªè²ï¼Œé¿å…æ¯æ­¥éƒ½æ˜¯ç¨ç«‹ç™½å™ªè²ï¼Œè®“æ¢ç´¢åœ¨å¯¦éš›æ§åˆ¶ä¸Šæ›´åˆç†ã€‚
#mu: OU éç¨‹çš„é•·æœŸå¹³å‡ï¼ˆmeanï¼‰æˆ–å¹³è¡¡å€¼ï¼ˆç›®æ¨™å€¼ï¼‰ã€‚åœ¨æ²’æœ‰å¤–åŠ›æ™‚ï¼Œx æœƒå¾€ mu å›æ­¸ã€‚
#sigma: éš¨æ©Ÿæ“¾å‹•å¼·åº¦ï¼ˆå™ªè²å¹…åº¦ï¼‰ï¼Œè¶Šå¤§æ³¢å‹•è¶Šå¤§ã€‚
#theta: å›æ­¸ä¿‚æ•¸ï¼ˆmean reversion rateï¼‰ï¼Œå€¼è¶Šå¤§ä»£è¡¨åé›¢ mu æ™‚æœƒæ›´å¿«æ‹‰å›ä¾†ã€‚
#dt: æ™‚é–“æ­¥é•·ï¼ˆé›¢æ•£åŒ–çš„æ™‚é–“é–“éš”ï¼‰ã€‚è‹¥ä½ çš„ agent æ¯æ¬¡æ›´æ–°ä¸æ˜¯ 1 ç§’ï¼Œå¯ä»¥ç”¨å¯¦éš›æ™‚é–“é–“éš”ï¼›å¸¸å¸¸ç‚º 1ã€‚
#x_prev: ä¿å­˜ä¸Šä¸€æ™‚é–“æ­¥çš„å™ªè²å€¼ï¼ˆOU éç¨‹æ˜¯æœ‰è¨˜æ†¶çš„ï¼‰ï¼Œåˆå§‹è¨­ç‚º 0.
#
#mu(Î¼) = 0.0ï¼šå° actions å¸¸è¦‹ï¼Œå™ªè²å¹³å‡ç‚º 0ã€‚ 
#theta(Î¸)ï¼š0.1 ~ 0.3ï¼ˆå¦‚æœå¸Œæœ›å¿«é€Ÿå›æ­¸å¯è¨­æ›´é«˜ï¼‰ã€‚
#sigma(Ïƒ)ï¼š0.1 ~ 0.6 å¸¸è¦‹ï¼›é–‹å§‹æ™‚å¤§ä¸€é»ï¼ˆä¾‹å¦‚ 0.3~0.6ï¼‰ï¼Œå¾ŒæœŸå¯ anneal åˆ° 0.01ã€‚
#dtï¼šå¦‚æœæ¯æ­¥æ›´æ–°ç‚º 1ï¼Œè¨­ 1ï¼›è‹¥æ¯ step è¡¨ç¤º 0.02sï¼Œè¨­ 0.02ï¼ˆä¸¦è®“å™ªè²å¹…åº¦èˆ‡æ™‚é–“æ­¥é•·ä¸€è‡´ï¼‰ã€‚
#
# Î¼ â€” å¹³è¡¡é» / é•·æœŸç›®æ¨™       ç‰©ç†ä¸Šï¼šé¡ä¼¼ã€Œé˜»å°¼å½ˆç°§çš„å¹³è¡¡ä½ç½®ã€
# Î¸ â€” é˜»å°¼/å›æ­¸é€Ÿåº¦          è¶Šå¤§ï¼šç³»çµ±è¿…é€Ÿè¢«æ‹‰å› Î¼ å™ªè²è®Šå¾—å›æ­¸æ›´å¿«ã€è®Šå‹•è¼ƒå°
# Ïƒ â€” ç†±æ“¾å‹•å¼·åº¦             è¶Šå¤§ï¼šå™ªè²å¹…åº¦è¶Šå¤§ æ¢ç´¢æ›´æ¿€é€²
# dt â€” æ™‚é–“å°ºåº¦              å¦‚æœæ¨¡æ“¬æ¯ä¸€æ­¥ä»£è¡¨æ›´å°çš„æ™‚é–“ï¼š å™ªè²æ”¾å¤§æ–¹å¼æœƒä¸åŒ æ–¹å·®å°æ‡‰ ğ‘‘ğ‘¡ dt
#
#OU å™ªè²æ˜¯ä¸€ç¨®æ¨¡æ“¬ã€Œå½ˆç°§ + éš¨æ©Ÿæ’æ“Šã€çš„éç¨‹
#
class OUActionNoise:
    def __init__(self, mu=0.0, sigma=0.5, theta=0.15, dt=1.0):
        self.mu = mu; self.sigma = sigma; self.theta = theta; self.dt = dt
        self.x_prev = 0.0
    def __call__(self):
        x = self.x_prev + self.theta*(self.mu - self.x_prev)*self.dt + self.sigma*math.sqrt(self.dt)*np.random.randn()
        self.x_prev = x
        return x
    #reset()ï¼šåœ¨æ¯å€‹ episode é–‹å§‹æ™‚æŠŠéç¨‹é‡ç½®ï¼ˆé€šå¸¸è¦é€™éº¼åšï¼Œå¦å‰‡å™ªè²æœƒè·¨è¶Š episodeï¼‰
    def reset(self): self.x_prev = 0.0 
    def set_sigma(self, sigma): self.sigma = sigma

# ---------- soft update ----------
def soft_update(target, source, tau):
    for t,p in zip(target.parameters(), source.parameters()):
        t.data.copy_(t.data * (1.0 - tau) + p.data * tau)

# ---------- training function ----------
def train_ddpg_stable(num_episodes=1000, episode_len=400, batch_size=128):
    state_dim = 4  # [omega, target_speed, delta_error,error_sum]
    actor = Actor(state_dim, action_max=24.0).to(device)
    critic = Critic(state_dim).to(device)
    actor_t = Actor(state_dim, action_max=24.0).to(device)
    critic_t = Critic(state_dim).to(device)
    actor_t.load_state_dict(actor.state_dict())
    critic_t.load_state_dict(critic.state_dict())

    actor_opt = optim.Adam(actor.parameters(), lr=1e-4)
    critic_opt = optim.Adam(critic.parameters(), lr=1e-3)

    buffer = ReplayBuffer()
    noise = OUActionNoise(sigma=0.6)  # initial exploration sigma è¶Šå¤§ï¼šå™ªè²å¹…åº¦è¶Šå¤§ æ¢ç´¢æ›´æ¿€é€²
    gamma = 0.99; tau_soft = 0.005
    Ts = 0.01  # Assuming DifferentiableMotor default Ts=0.01

    rewards_log = []
    u_max_log = []
    u_mean_log = []
    error_log = []
    
    # å¹…å€¼
    A_min=0.1
    A_max=0.5
    
    #åç½®ï¼Œä¸­å¿ƒç‚¹åœ¨ 1.0
    O_min=-0.3
    O_max=0.3
    
    # é¢‘ç‡/å‘¨æœŸéšæœºåŒ–èŒƒå›´
    T_min = 3.0
    T_max = 100.0

    #è¯¾ç¨‹å­¦ä¹  (Curriculum Learning)
    for ep in range(1, num_episodes+1):
        # curriculum: first few episodes fixed params, then randomize
        if ep <= 100:
            K, tau_m, zeta = 10.0, 0.5, 0.7
            
            T_cycle = float(T_min+T_max)/2.0
            omega_freq = 2 * math.pi / T_cycle
            amplitude = float(A_min+A_max)/2.0
            offset = float(O_min+O_max)/2.0 

        elif ep <= 200:
            K, tau_m, zeta = 10.0, 0.5, 0.7

            T_cycle = float(np.random.uniform(T_min, T_max))
            omega_freq = 2 * math.pi / T_cycle
            amplitude = float(A_min+A_max)/2.0
            offset = float(O_min+O_max)/2.0                
        elif ep <= 300:
            K, tau_m, zeta = 10.0, 0.5, 0.7
            
            T_cycle = float(np.random.uniform(T_min, T_max))
            omega_freq = 2 * math.pi / T_cycle
            amplitude = float(np.random.uniform(A_min, A_max))
            offset = float(O_min+O_max)/2.0  
        else:
            K, tau_m, zeta = 10.0, 0.5, 0.7
	
            T_cycle = float(np.random.uniform(T_min, T_max))
            omega_freq = 2 * math.pi / T_cycle
            amplitude = float(np.random.uniform(A_min, A_max))
            offset = float(np.random.uniform(O_min, O_max))     
        
        # åˆå§‹ç›®æ ‡é€Ÿåº¦ (t=0 æ—¶ sin(0)=0)
        initial_target_speed = float(amplitude * math.sin(0) + offset)
        target_speed = initial_target_speed
        

        env = DifferentiableMotor(K=K, tau=tau_m, zeta=zeta, Ts=Ts)
        env.reset()
        

        error_sum = 0.0  # ç´¯ç§¯è¯¯å·® I é¡¹ (ç§¯åˆ†é¡¹)
        ep_reward = 0.0
        noise.reset()

        # decay noise sigma across episodes
        sigma_decay = max(0.05, 0.6 * (1.0 - (ep/num_episodes)))
        noise.set_sigma(sigma_decay)

        # stats
        episode_us = []
        
        #[omega, target_speed, delta_error, error_sum]
        state = np.array([env.omega, target_speed, 0.0,error_sum], dtype=np.float32)
        prev_error = target_speed - env.omega # åˆå§‹è¯¯å·®

        for t in range(episode_len):
            
            # *****************************************
            # --- ç›®æ ‡é€Ÿåº¦åŠ¨æ€æ›´æ–° ---
            current_time = t * Ts
            target_speed = float(amplitude * math.sin(omega_freq * current_time) + offset)
            target_speed = target_speed
            # ------------------------
            
            s_t = torch.tensor(state.reshape(1,-1), dtype=torch.float32, device=device)
            with torch.no_grad():
                a_det = actor(s_t).cpu().numpy().flatten()[0]

            # exploration: add noise during training
            a_t = a_det + noise()
            a_t = float(np.clip(a_t, -actor.action_max, actor.action_max))
            episode_us.append(a_t)

            # step env
            omega_next = env.step(a_t)
            
            # è¯¯å·®è®¡ç®—åŸºäºæœ€æ–°çš„ target_speed
            error = target_speed - omega_next
            error_sum += error * Ts # ç´¯ç§¯ç§¯åˆ†è¯¯å·®
            delta_error = error - prev_error

            # reward: å¼ºè°ƒ error reduction (prev_error - error), small penalty on magnitude
            # ä¸ºäº†æé«˜è·Ÿè¸ªç²¾åº¦ï¼Œå‡å°‘äº†æ§åˆ¶é‡æƒ©ç½š (0.0005 -> 0.0001)
            #r = 2.0 * (prev_error - error) - 0.8 * abs(error) - 0.0001 * (a_t**2)
            #r = 2.0 * (prev_error - error) - 0.8 * abs(error) - 0.001 * abs(error_sum) - 0.0001 * (a_t**2)
            r = -2.0 * abs(delta_error) - 0.8 * abs(error) - 0.001 * abs(error_sum) - 0.0001 * abs(a_t)
            r = float(np.clip(r, -10.0, 10.0))

            # next_state å¿…é¡»åŒ…å«å½“å‰çš„ç›®æ ‡é€Ÿåº¦
            next_state = np.array([omega_next, target_speed, delta_error,error_sum], dtype=np.float32)
            
            #(state, action, reward, next_state, done)
            buffer.push(state, 
                        np.array([a_t], dtype=np.float32), 
                        np.array([r], dtype=np.float32), 
                        next_state, 
                        False)

            state = next_state
            prev_error = error # æ›´æ–° prev_error ä»¥ä¾›ä¸‹ä¸€è½®ä½¿ç”¨
            ep_reward += r

            # learning step
            if len(buffer) >= batch_size:
                #(state, action, reward, next_state, done)
                s_b, a_b, r_b, s2_b, d_b = buffer.sample(batch_size)
                with torch.no_grad():
                    a2 = actor_t(s2_b)
                    q_next = critic_t(s2_b, a2)
                    q_target = r_b + (1.0 - d_b) * gamma * q_next

                q_val = critic(s_b, a_b)
                critic_loss = nn.MSELoss()(q_val, q_target)
                critic_opt.zero_grad(); critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(critic.parameters(), 1.0)
                critic_opt.step()

                actor_loss = -critic(s_b, actor(s_b)).mean()
                actor_opt.zero_grad(); actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(actor.parameters(), 1.0)
                actor_opt.step()

                soft_update(actor_t, actor, tau_soft)
                soft_update(critic_t, critic, tau_soft)

        error_log.append(error)
        rewards_log.append(ep_reward)
        u_max_log.append(max(episode_us) if episode_us else 0.0)
        u_mean_log.append(np.mean(episode_us) if episode_us else 0.0)

        if ep % 10 == 0:
            print(f"Ep {ep}/{num_episodes} reward {ep_reward:.3f}  u_max={u_max_log[-1]:.3f} u_mean={u_mean_log[-1]:.3f} noise_sigma={noise.sigma:.3f} error={error_log[-1]:.3f}")

        if ep % 200 == 0:
            torch.save(actor.state_dict(), "ddpg_actor_stable_sin.pth") # æ›´æ”¹ä¿å­˜æ–‡ä»¶å

    torch.save(actor.state_dict(), "ddpg_actor_stable_sin.pth") # æ›´æ”¹ä¿å­˜æ–‡ä»¶å
    print("Training finished. Actor saved as ddpg_actor_stable_sin.pth")

    # plot reward and u stats
    plt.figure(figsize=(12,4))
    plt.subplot(1,2,1)
    plt.plot(rewards_log); plt.xlabel("episode"); plt.ylabel("episode reward"); plt.grid(True)
    plt.subplot(1,2,2)
    plt.plot(u_max_log, label="u_max"); plt.plot(u_mean_log, label="u_mean"); plt.xlabel("episode"); plt.legend(); plt.grid(True)
    plt.show()
    return actor

if __name__ == "__main__":
    # æ¨èå¢åŠ è®­ç»ƒé›†æ•°ä»¥é€‚åº”åŠ¨æ€ç›®æ ‡è¿½è¸ª
    train_ddpg_stable(num_episodes=400, episode_len=400, batch_size=64)